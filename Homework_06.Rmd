---
title: "Homework_06"
author: "Molly Ratliff"
date: "2024-02-21"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**1) Run the sample code. Set up a new .Rmd file for this exercise. Copy and paste the code below into different code chunks, and then read the text and run the code chunks one at a time to see what they do. You probably won’t understand everything in the code, but this is a good start for seeing some realistic uses of ggplot. We will cover most of these details in the next few weeks.**

Sample code was successfully ran.

**2) Try it with your own data. Once the code is in and runs, try running this analysis on your own data (or data from your lab, or data from a published paper; if you’re stumped, check out publicly available data sets on Dryad, ESA, or the LTER Network). Find a vector of data (of any size), set it up in a .csv file, and read the data into a data frame with this code chunk:**

```{r eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE}
# Libraries
library(ggplot2)
library(MASS)
library(dplyr)

# Read in data
z <- read.csv("Compiled_DSWR_Fluxdata.csv")
z$myVar <- z$fn2o_lin

# View original histogram
p1 <- ggplot(data=z, aes(x=myVar, y=..density..)) +
  geom_histogram(color="grey60",fill="cornsilk",size=0.2) 
# Log transform to put less weight on values near zero
z <- z%>%
  mutate(myVar=log(fn2o_lin))%>%
  filter(myVar>0)
# View new histogram
p1 <- ggplot(data=z, aes(x=myVar, y=..density..)) +
  geom_histogram(color="grey60",fill="cornsilk",size=0.2) 

# Add empirical density curve
p1 <-  p1 +  geom_density(linetype="dotted",size=0.75)

# Maximum likelihood parameters for normal
normPars <- fitdistr(z$myVar,"normal")

# Plot normal probability density 
meanML <- normPars$estimate["mean"]
sdML <- normPars$estimate["sd"]
xval <- seq(0,max(z$myVar),len=length(z$myVar))
stat <- stat_function(aes(x = xval, y = ..y..), fun = dnorm, colour="red", n = length(z$myVar), args = list(mean = meanML, sd = sdML))

# Plot exponential probability density
expoPars <- fitdistr(z$myVar,"exponential")
rateML <- expoPars$estimate["rate"]
stat2 <- stat_function(aes(x = xval, y = ..y..), fun = dexp, colour="blue", n = length(z$myVar), args = list(rate=rateML))

# Plot uniform probability density
stat3 <- stat_function(aes(x = xval, y = ..y..), fun = dunif, colour="darkgreen", n = length(z$myVar), args = list(min=min(z$myVar), max=max(z$myVar)))
```

```{r warning=FALSE, message=FALSE}
# Plot gamma probability density
gammaPars <- fitdistr(z$myVar,"gamma")
shapeML <- gammaPars$estimate["shape"]
rateML <- gammaPars$estimate["rate"]
stat4 <- stat_function(aes(x = xval, y = ..y..), fun = dgamma, colour="brown", n = length(z$myVar), args = list(shape=shapeML, rate=rateML))
p1 + stat + stat2 + stat3 + stat4

# Plot beta probability density
pSpecial <- ggplot(data=z, aes(x=myVar/(max(myVar + 0.1)), y=..density..)) +
  geom_histogram(color="grey60",fill="cornsilk",size=0.2) + 
  xlim(c(0,1)) +
  geom_density(size=0.75,linetype="dotted")
betaPars <- fitdistr(x=z$myVar/max(z$myVar + 0.1),start=list(shape1=1,shape2=2),"beta")
shape1ML <- betaPars$estimate["shape1"]
shape2ML <- betaPars$estimate["shape2"]
statSpecial <- stat_function(aes(x = xval, y = ..y..), fun = dbeta, colour="orchid", n = length(z$myVar), args = list(shape1=shape1ML,shape2=shape2ML))
pSpecial + statSpecial
```

**3) Find best-fitting distribution. Take a look at the second-to-last graph which shows the histogram of your data and 4 probability density curves (normal, uniform, exponential, gamma) that are fit to the data. Find the best-fitting distribution for your data. For most data sets, the gamma will probably fit best, but if you data set is small, it may be very hard to see much of a difference between the curves. The beta distribution in the final graph is somewhat special. It often fits the data pretty well, but that is because we have assumed the largest data point is the true upper bound, and everything is scaled to that. The fit of the uniform distribution also fixes the upper bound. The other curves (normal, exponential, and gamma) are more realistic because they do not have an upper bound.**

The normal probability distribution appears to fit the data the best. The bimodal nature of the data is likely due to various drivers of nitrous oxide fluxes interacting. Before the data was log transformed, the majority of the data was near zero. This is due to the relatively low activity level of nitrogen fixing soil microorganisms when N (from manure) is not present in the soil. The high but infrequent nitrous oxide fluxes are likely due to pulses post-manure application. However, nitrous oxide pulses can also occur following precipitation events and under high soil temperature conditions.


Due to the interactive nature of both environmental variables (soil temperature and soil moisture), and soil management practices (manure application), this does seem to be an accurate depiction of the data. The lack of data in the middle of the distribution is likely due to the lack of medium size nitrous oxide fluxes. Fluxes typically are baseline values around zero, or significant, high pulses influenced by an environmental variable or soil amendment.

**4) Simulate a new data set. Using the best-fitting distribution, go back to the code and get the maximum likelihood parameters. Use those to simulate a new data set, with the same length as your original vector, and plot that in a histogram and add the probability density curve. Right below that, generate a fresh histogram plot of the original data, and also include the probability density curve.**
```{r warning=FALSE, message=FALSE}
# Simulate a new data site using maximum likelihood parameters
# mean
# 1.78450304   
# sd
# 1.13407071 

new <- rnorm(n=300,mean=1.78,sd=1.13)
new <- data.frame(1:300,new)
p1 <- ggplot(data=new, aes(x=new, y=..density..)) +
  geom_histogram(color="grey60",fill="cornsilk",size=0.2) 
print(p1)

p1old <- ggplot(data=z, aes(x=myVar, y=..density..)) +
  geom_histogram(color="grey60",fill="cornsilk",size=0.2) 
print(p1old)
```


The histogram for the simulated data and the real data have some similarities and some differences. The original data excluded zeros from the analysis. If zeros were included, the skew may be more similar to the simulated data. The original data may also have high, positive outliers that should be excluded. Additional pre-processing may be helpful to tease out a better way to subset the data (such as time, treatment, or location).